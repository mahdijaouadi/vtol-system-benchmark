{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import socket\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=5\n",
    "GAMMA=0.99\n",
    "MAX_STEPS=100\n",
    "EPS=0.99\n",
    "ALPHA=0.01\n",
    "FREQ_UPDATE=30\n",
    "replay_memory1=[]\n",
    "replay_memory2=[]\n",
    "dt=0.1\n",
    "BATCH_SIZE=16\n",
    "critical_angle=75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.bn_input=nn.BatchNorm1d(input_size)\n",
    "\n",
    "        self.fc1=nn.Linear(input_size, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2=nn.Linear(64, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.output_fc=nn.Linear(64, 3)\n",
    "    def forward(self, x):\n",
    "        out=self.bn_input(x)\n",
    "        out=F.relu(self.fc1(out))\n",
    "        out=self.bn1(out)\n",
    "        out=F.relu(self.fc2(out))\n",
    "        out=self.bn2(out)\n",
    "        out=self.output_fc(out)\n",
    "        return out\n",
    "    def act(self,obs):\n",
    "        self.eval()\n",
    "        q_values=self(obs.to(device)).to('cpu')\n",
    "        max_q_index=torch.argmax(q_values)\n",
    "        action=max_q_index.detach().item()\n",
    "        self.train()\n",
    "        return action\n",
    "    \n",
    "\n",
    "policy_model1 = Agent(input_size=4).to(device)\n",
    "target_model1 = Agent(input_size=4).to(device)\n",
    "target_model1.load_state_dict(policy_model1.state_dict())\n",
    "\n",
    "policy_model2 = Agent(input_size=4).to(device)\n",
    "target_model2 = Agent(input_size=4).to(device)\n",
    "target_model2.load_state_dict(policy_model2.state_dict())\n",
    "\n",
    "\n",
    "optimizer1=torch.optim.Adam(policy_model1.parameters(), lr=5e-3,weight_decay=5e-5)\n",
    "optimizer2=torch.optim.Adam(policy_model2.parameters(), lr=5e-3,weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PIDController:\n",
    "    def __init__(self, kp, ki, kd, setpoint):\n",
    "        self.kp = kp\n",
    "        self.ki = ki\n",
    "        self.kd = kd\n",
    "        self.setpoint = setpoint\n",
    "        self.integral = 0\n",
    "        self.previous_error = 0\n",
    "\n",
    "    def update(self, current_value, dt):\n",
    "        error = self.setpoint - current_value\n",
    "        self.integral += error * dt\n",
    "        derivative = (error - self.previous_error) / dt\n",
    "        output = (self.kp * error) + (self.ki * self.integral) + (self.kd * derivative)\n",
    "        self.previous_error = error\n",
    "        return output\n",
    "\n",
    "class VirtualMotorPropeller:\n",
    "    def __init__(self, R, Ke, Kt, thrust_coefficient, voltage_limit):\n",
    "        self.angular_velocity = 0.0\n",
    "        self.moment_inertia=0.012\n",
    "        self.R = R\n",
    "        self.Ke = Ke\n",
    "        self.Kt = Kt\n",
    "        self.thrust_coefficient = thrust_coefficient\n",
    "        self.voltage_limit = voltage_limit\n",
    "        self.current = 0.0\n",
    "\n",
    "    def apply_voltage(self, voltage, dt):\n",
    "        voltage = max(min(voltage, self.voltage_limit), -self.voltage_limit)\n",
    "        back_emf = self.Ke * self.angular_velocity\n",
    "        self.current = (voltage - back_emf) / self.R\n",
    "        torque = self.Kt * self.current\n",
    "        angular_acceleration = torque/self.moment_inertia\n",
    "        self.angular_velocity += angular_acceleration * dt\n",
    "\n",
    "\n",
    "    def get_thrust(self):\n",
    "        return self.thrust_coefficient * (self.angular_velocity ** 2)\n",
    "# Seesaw Model Class\n",
    "class Seesaw:\n",
    "    def __init__(self, J, arm_length, damping):\n",
    "        self.angle = 0.0\n",
    "        self.angular_velocity = 0.0\n",
    "        self.J = J \n",
    "        self.arm_length = arm_length\n",
    "        self.damping = damping \n",
    "\n",
    "    def apply_forces(self, thrust_left, thrust_right, dt):\n",
    "        net_torque = (self.arm_length/2) * (thrust_right - thrust_left)\n",
    "        gravity_torque = 0.3 * 9.8 * (self.arm_length / 2) * np.sin(self.angle)\n",
    "        total_torque = net_torque - gravity_torque\n",
    "        angular_acceleration = (total_torque - self.damping * self.angular_velocity) / self.J\n",
    "        self.angular_velocity += angular_acceleration * dt\n",
    "        self.angle += self.angular_velocity * dt\n",
    "\n",
    "    def get_angle_deg(self):\n",
    "        return self.angle * (180.0 / np.pi)\n",
    "\n",
    "class Gyroscope:\n",
    "    def __init__(self, noise_std=0.0):\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def read_angle(self, true_angle):\n",
    "        noise = np.random.normal(0, self.noise_std)\n",
    "        return true_angle + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model1(replay_memory):\n",
    "    replay_memory=torch.tensor(replay_memory, dtype=torch.float32)\n",
    "    state=replay_memory[:,:4]\n",
    "    next_state=replay_memory[:,4:8]\n",
    "    rewards=replay_memory[:,8].unsqueeze(1)\n",
    "    actions=replay_memory[:,9].to(torch.int64).unsqueeze(1)\n",
    "    dones=replay_memory[:,10].unsqueeze(1)\n",
    "    target_q_values=target_model1(next_state.to(device)).to('cpu')\n",
    "    max_target_q_values=target_q_values.max(dim=1,keepdim=True)[0]\n",
    "    targets=rewards+GAMMA*(1-dones)*max_target_q_values\n",
    "\n",
    "    q_values=policy_model1(state.to(device)).to('cpu')\n",
    "    action_q_values=torch.gather(input=q_values,dim=1,index=actions)\n",
    "    loss=nn.functional.mse_loss(action_q_values, targets)\n",
    "    optimizer1.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_model1.parameters(), max_norm=1.0)\n",
    "    optimizer1.step()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def training_model2(replay_memory):\n",
    "    replay_memory=torch.tensor(replay_memory, dtype=torch.float32)\n",
    "    state=replay_memory[:,:4]\n",
    "    next_state=replay_memory[:,4:8]\n",
    "    rewards=replay_memory[:,8].unsqueeze(1)\n",
    "    actions=replay_memory[:,9].to(torch.int64).unsqueeze(1)\n",
    "    dones=replay_memory[:,10].unsqueeze(1)\n",
    "    target_q_values=target_model2(next_state.to(device)).to('cpu')\n",
    "    max_target_q_values=target_q_values.max(dim=1,keepdim=True)[0]\n",
    "    targets=rewards+GAMMA*(1-dones)*max_target_q_values\n",
    "\n",
    "    q_values=policy_model2(state.to(device)).to('cpu')\n",
    "    action_q_values=torch.gather(input=q_values,dim=1,index=actions)\n",
    "    loss=nn.functional.mse_loss(action_q_values, targets)\n",
    "    optimizer2.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_model2.parameters(), max_norm=1.0)\n",
    "    optimizer2.step()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_environment():\n",
    "\n",
    "    R = 0.5\n",
    "    Ke = 0.02\n",
    "    Kt = 0.02\n",
    "    thrust_coefficient = 1e-5\n",
    "    voltage_limit = 24.0\n",
    "\n",
    "    J = 0.012\n",
    "    arm_length = 0.4\n",
    "    damping = 0.05\n",
    "\n",
    "    left_motor = VirtualMotorPropeller(R, Ke, Kt, thrust_coefficient, voltage_limit)\n",
    "    right_motor = VirtualMotorPropeller(R, Ke, Kt, thrust_coefficient, voltage_limit)\n",
    "    seesaw = Seesaw(J, arm_length, damping)\n",
    "    gyro = Gyroscope(noise_std=0.0)\n",
    "    return left_motor,right_motor,seesaw,gyro,voltage_limit\n",
    "\n",
    "left_motor,right_motor,seesaw,gyro,voltage_limit=initialize_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "voltage_left=0\n",
    "voltage_right=0\n",
    "def run_step_simulation(control_signal_right,control_signal_left):\n",
    "    global voltage_limit\n",
    "    global dt\n",
    "    global seesaw\n",
    "    global left_motor\n",
    "    global right_motor\n",
    "    global voltage_left\n",
    "    global voltage_right\n",
    "    voltage_right = np.clip(voltage_right+control_signal_right, 0, voltage_limit)\n",
    "    voltage_left = np.clip(voltage_left+control_signal_left, 0, voltage_limit)\n",
    "\n",
    "    left_motor.apply_voltage(voltage_left, dt)\n",
    "    right_motor.apply_voltage(voltage_right, dt)\n",
    "\n",
    "    thrust_left = left_motor.get_thrust()\n",
    "    thrust_right = right_motor.get_thrust()\n",
    "\n",
    "    seesaw.apply_forces(thrust_left, thrust_right, dt)\n",
    "\n",
    "\n",
    "def step(state):\n",
    "    global gyro\n",
    "    target_angle_degree=state[0]\n",
    "    state=torch.tensor([state], dtype=torch.float32)\n",
    "\n",
    "    if random.uniform(0,1)>EPS:\n",
    "        action1=policy_model1.act(state)\n",
    "        action2=policy_model2.act(state)\n",
    "    else:\n",
    "        action1=random.randint(0,2)\n",
    "        action2=random.randint(0,2)\n",
    "\n",
    "    if action1==0:\n",
    "        control_signal_right=-12\n",
    "    elif action1==1:\n",
    "        control_signal_right=12\n",
    "    else:\n",
    "        control_signal_right=0\n",
    "    \n",
    "    if action2==0:\n",
    "        control_signal_left=-12\n",
    "    elif action2==1:\n",
    "        control_signal_left=12\n",
    "    else:\n",
    "        control_signal_left=0\n",
    "\n",
    "    run_step_simulation(control_signal_right=control_signal_right,control_signal_left=control_signal_left)\n",
    "    state=[target_angle_degree,gyro.read_angle(seesaw.get_angle_deg()),left_motor.get_thrust(),right_motor.get_thrust()]\n",
    "    reward=-abs(target_angle_degree-gyro.read_angle(seesaw.get_angle_deg()))\n",
    "    return state,reward,action1,reward,action2\n",
    "def play_episode(state):\n",
    "    global gyro\n",
    "    for i in range(MAX_STEPS):\n",
    "        old_state=state.copy()\n",
    "        state,rew_t1,action_t1, rew_t2, action_t2=step(state)\n",
    "\n",
    "        obs1=old_state.copy()\n",
    "        obs1.extend(state)\n",
    "        obs1.append(rew_t1)\n",
    "        obs1.append(action_t1)\n",
    "\n",
    "        obs2=old_state.copy()\n",
    "        obs2.extend(state)\n",
    "        obs2.append(rew_t2)\n",
    "        obs2.append(action_t2)\n",
    "\n",
    "        if i==MAX_STEPS-1 or abs(gyro.read_angle(seesaw.get_angle_deg()))>=critical_angle:\n",
    "            obs1.append(1)\n",
    "            obs2.append(1)\n",
    "            replay_memory1.append(obs1)\n",
    "            replay_memory2.append(obs2)\n",
    "        else:\n",
    "            obs1.append(0)\n",
    "            obs2.append(0)\n",
    "            replay_memory1.append(obs1)\n",
    "            replay_memory2.append(obs2)\n",
    "    return state,rew_t1,rew_t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_episodes=100000\n",
    "reward_avg_logs1=[0]\n",
    "reward_avg_logs2=[0]\n",
    "for i in range(number_episodes):\n",
    "    left_motor,right_motor,seesaw,gyro,voltage_limit=initialize_environment()\n",
    "    target_angle_deg=random.uniform(-60,60)\n",
    "    measured_angle_deg = gyro.read_angle(seesaw.get_angle_deg())\n",
    "\n",
    "    state=[target_angle_deg,measured_angle_deg,left_motor.get_thrust(),right_motor.get_thrust()]\n",
    "    state,rew_t1,rew_t2=play_episode(state)\n",
    "    EPS=EPS*0.99999\n",
    "    reward_avg_logs1.append((len(reward_avg_logs1)*reward_avg_logs1[-1]+rew_t1)/(len(reward_avg_logs1)+1))\n",
    "    reward_avg_logs2.append((len(reward_avg_logs2)*reward_avg_logs2[-1]+rew_t2)/(len(reward_avg_logs2)+1))\n",
    "    training_model1(replay_memory=random.sample(replay_memory1, BATCH_SIZE))\n",
    "    training_model2(replay_memory=random.sample(replay_memory2, BATCH_SIZE))\n",
    "\n",
    "    if (i+1)%FREQ_UPDATE==0:\n",
    "        target_model1.load_state_dict(policy_model1.state_dict())\n",
    "        target_model2.load_state_dict(policy_model2.state_dict())\n",
    "        print(\"Last reward is:\",reward_avg_logs2[-1],\" and epsilon= \",EPS)\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(policy_model1.state_dict(), 'agent1.pth')\n",
    "torch.save(policy_model2.state_dict(), 'agent2.pth')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
